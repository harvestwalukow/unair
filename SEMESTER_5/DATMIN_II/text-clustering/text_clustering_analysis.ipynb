{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Clustering Analysis - adaKami Reviews\n",
        "## Dataset: adaKami-reviews.csv\n",
        "\n",
        "Analisis ini mencakup:\n",
        "1. Preprocessing\n",
        "2. WordCloud\n",
        "3. Clustering dengan Sentence-Transformers Embeddings (all-mpnet-base-v2)\n",
        "4. Visualisasi\n",
        "5. Silhouette Score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries dan Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text processing and embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "# For interactive plots\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\n",
        "df = pd.read_csv('adaKami-reviews.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTotal reviews: {len(df)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Indonesian stopwords\n",
        "indonesian_stopwords = set([\n",
        "    'yang', 'di', 'dan', 'ini', 'itu', 'dengan', 'untuk', 'pada', 'adalah', 'dari',\n",
        "    'ke', 'dalam', 'oleh', 'tidak', 'akan', 'dapat', 'ada', 'atau', 'juga', 'saya',\n",
        "    'sudah', 'jika', 'saat', 'telah', 'seperti', 'bisa', 'semua', 'lebih', 'sangat',\n",
        "    'hanya', 'karena', 'belum', 'masih', 'mereka', 'kita', 'dia', 'kami', 'anda',\n",
        "    'nya', 'apa', 'siapa', 'kapan', 'dimana', 'bagaimana', 'mengapa', 'saja',\n",
        "    'baru', 'punya', 'mau', 'pernah', 'setelah', 'selalu', 'lagi', 'sama', 'sebagai',\n",
        "    'kalau', 'bila', 'tapi', 'tetapi', 'walaupun', 'sedang', 'antara', 'sekitar',\n",
        "    'ketika', 'sebelum', 'maka', 'tersebut', 'bahkan', 'melalui', 'hingga', 'agar',\n",
        "    'namun', 'lalu', 'kemudian', 'jadi', 'apakah', 'ia', 'mereka', 'ataupun'\n",
        "])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocessing text untuk bahasa Indonesia:\n",
        "    - Lowercase\n",
        "    - Remove URLs\n",
        "    - Remove mentions dan hashtags\n",
        "    - Remove special characters\n",
        "    - Remove extra whitespace\n",
        "    - Remove stopwords\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string and lowercase\n",
        "    text = str(text).lower()\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    \n",
        "    # Remove mentions and hashtags\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "    \n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    \n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Remove stopwords\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in indonesian_stopwords and len(word) > 2]\n",
        "    \n",
        "    return ' '.join(words)\n",
        "\n",
        "print(\"Preprocessing function defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply preprocessing\n",
        "print(\"Starting preprocessing...\")\n",
        "df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
        "\n",
        "# Remove empty reviews\n",
        "df = df[df['cleaned_review'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "print(f\"Preprocessing complete!\")\n",
        "print(f\"Total reviews after cleaning: {len(df)}\")\n",
        "print(\"\\nExample of original vs cleaned text:\")\n",
        "print(f\"\\nOriginal: {df['review'].iloc[0][:200]}...\")\n",
        "print(f\"\\nCleaned: {df['cleaned_review'].iloc[0][:200]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. WordCloud Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine all cleaned text\n",
        "all_text = ' '.join(df['cleaned_review'])\n",
        "\n",
        "# Create wordcloud\n",
        "wordcloud = WordCloud(\n",
        "    width=1600, \n",
        "    height=800,\n",
        "    background_color='white',\n",
        "    colormap='viridis',\n",
        "    max_words=100,\n",
        "    relative_scaling=0.5,\n",
        "    min_font_size=10\n",
        ").generate(all_text)\n",
        "\n",
        "# Plot wordcloud\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('WordCloud - adaKami Reviews', fontsize=20, fontweight='bold', pad=20)\n",
        "plt.tight_layout(pad=0)\n",
        "plt.savefig('wordcloud.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"WordCloud generated and saved as 'wordcloud.png'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top 20 most frequent words\n",
        "from collections import Counter\n",
        "\n",
        "words = all_text.split()\n",
        "word_freq = Counter(words)\n",
        "top_words = word_freq.most_common(20)\n",
        "\n",
        "# Create bar plot\n",
        "words_df = pd.DataFrame(top_words, columns=['Word', 'Frequency'])\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=words_df, x='Frequency', y='Word', palette='viridis')\n",
        "plt.title('Top 20 Most Frequent Words', fontsize=16, fontweight='bold')\n",
        "plt.xlabel('Frequency', fontsize=12)\n",
        "plt.ylabel('Word', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig('top_words.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 20 words:\")\n",
        "print(words_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Sentence-Transformers Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Sentence-Transformer model\n",
        "print(\"Loading Sentence-Transformer model: all-mpnet-base-v2...\")\n",
        "print(\"This may take a moment on first run as the model needs to be downloaded...\")\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
        "\n",
        "# Create embeddings\n",
        "print(\"\\nGenerating embeddings for all reviews...\")\n",
        "print(\"This may take a few minutes depending on your hardware and dataset size...\")\n",
        "\n",
        "# Use original review text for better semantic understanding\n",
        "embeddings = model.encode(\n",
        "    df['review'].tolist(),\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
        "print(f\"Number of reviews: {embeddings.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Finding Optimal Number of Clusters (Elbow Method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Elbow method to find optimal k\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "K_range = range(2, 11)\n",
        "\n",
        "print(\"Computing elbow method and silhouette scores...\")\n",
        "for k in K_range:\n",
        "    print(f\"Testing k={k}...\", end=\" \")\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
        "    kmeans.fit(embeddings)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "    score = silhouette_score(embeddings, kmeans.labels_)\n",
        "    silhouette_scores.append(score)\n",
        "    print(f\"Silhouette Score: {score:.4f}\")\n",
        "\n",
        "# Plot elbow curve\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "# Elbow plot\n",
        "ax1.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "ax1.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "ax1.set_ylabel('Inertia', fontsize=12)\n",
        "ax1.set_title('Elbow Method For Optimal k', fontsize=14, fontweight='bold')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Silhouette score plot\n",
        "ax2.plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
        "ax2.set_xlabel('Number of Clusters (k)', fontsize=12)\n",
        "ax2.set_ylabel('Silhouette Score', fontsize=12)\n",
        "ax2.set_title('Silhouette Score For Different k', fontsize=14, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('elbow_silhouette.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Find optimal k\n",
        "optimal_k = K_range[silhouette_scores.index(max(silhouette_scores))]\n",
        "print(f\"\\nOptimal number of clusters based on silhouette score: {optimal_k}\")\n",
        "print(f\"Best silhouette score: {max(silhouette_scores):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. K-Means Clustering with Optimal k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use optimal k (or you can change this manually)\n",
        "n_clusters = optimal_k\n",
        "\n",
        "print(f\"Performing K-Means clustering with k={n_clusters}...\")\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)\n",
        "df['cluster'] = kmeans.fit_predict(embeddings)\n",
        "\n",
        "print(f\"\\nClustering complete!\")\n",
        "print(f\"\\nCluster distribution:\")\n",
        "print(df['cluster'].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Cluster Analysis & Top Terms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get top keywords for each cluster using word frequency\n",
        "from collections import Counter\n",
        "\n",
        "def get_top_keywords_by_frequency(n_terms=15):\n",
        "    \"\"\"Extract top keywords for each cluster based on word frequency\"\"\"\n",
        "    cluster_keywords = {}\n",
        "    \n",
        "    for i in range(n_clusters):\n",
        "        # Get all reviews in this cluster\n",
        "        cluster_reviews = df[df['cluster'] == i]['cleaned_review']\n",
        "        \n",
        "        # Combine all text and count word frequencies\n",
        "        all_text = ' '.join(cluster_reviews)\n",
        "        words = all_text.split()\n",
        "        word_freq = Counter(words)\n",
        "        \n",
        "        # Get top N words\n",
        "        top_words = [word for word, freq in word_freq.most_common(n_terms)]\n",
        "        cluster_keywords[i] = top_words\n",
        "    \n",
        "    return cluster_keywords\n",
        "\n",
        "top_keywords = get_top_keywords_by_frequency(n_terms=15)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TOP KEYWORDS FOR EACH CLUSTER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for cluster_id, keywords in top_keywords.items():\n",
        "    count = len(df[df['cluster'] == cluster_id])\n",
        "    print(f\"\\nCluster {cluster_id} ({count} reviews):\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\", \".join(keywords))\n",
        "    print(\"\\nSample reviews:\")\n",
        "    samples = df[df['cluster'] == cluster_id]['review'].head(3)\n",
        "    for idx, review in enumerate(samples, 1):\n",
        "        print(f\"  {idx}. {review[:150]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cluster statistics\n",
        "cluster_stats = df.groupby('cluster').agg({\n",
        "    'rating': ['mean', 'count'],\n",
        "    'review': lambda x: x.str.len().mean()\n",
        "}).round(2)\n",
        "\n",
        "cluster_stats.columns = ['Avg_Rating', 'Review_Count', 'Avg_Length']\n",
        "cluster_stats = cluster_stats.reset_index()\n",
        "\n",
        "print(\"\\nCluster Statistics:\")\n",
        "print(cluster_stats)\n",
        "\n",
        "# Visualize cluster statistics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Review count per cluster\n",
        "axes[0].bar(cluster_stats['cluster'], cluster_stats['Review_Count'], color='skyblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Cluster', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Reviews', fontsize=12)\n",
        "axes[0].set_title('Review Count per Cluster', fontsize=14, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Average rating per cluster\n",
        "axes[1].bar(cluster_stats['cluster'], cluster_stats['Avg_Rating'], color='lightcoral', edgecolor='black')\n",
        "axes[1].set_xlabel('Cluster', fontsize=12)\n",
        "axes[1].set_ylabel('Average Rating', fontsize=12)\n",
        "axes[1].set_title('Average Rating per Cluster', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Average review length per cluster\n",
        "axes[2].bar(cluster_stats['cluster'], cluster_stats['Avg_Length'], color='lightgreen', edgecolor='black')\n",
        "axes[2].set_xlabel('Cluster', fontsize=12)\n",
        "axes[2].set_ylabel('Average Review Length', fontsize=12)\n",
        "axes[2].set_title('Average Review Length per Cluster', fontsize=14, fontweight='bold')\n",
        "axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('cluster_statistics.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Dimensionality Reduction & Visualization (PCA)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reduce dimensions using PCA for visualization\n",
        "print(\"Applying PCA for 2D visualization...\")\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "pca_2d_result = pca_2d.fit_transform(embeddings)\n",
        "\n",
        "df['pca_1'] = pca_2d_result[:, 0]\n",
        "df['pca_2'] = pca_2d_result[:, 1]\n",
        "\n",
        "print(f\"Explained variance ratio: {pca_2d.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca_2d.explained_variance_ratio_):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Static 2D visualization with matplotlib\n",
        "plt.figure(figsize=(14, 10))\n",
        "scatter = plt.scatter(\n",
        "    df['pca_1'], \n",
        "    df['pca_2'], \n",
        "    c=df['cluster'], \n",
        "    cmap='viridis', \n",
        "    alpha=0.6,\n",
        "    s=50,\n",
        "    edgecolors='black',\n",
        "    linewidth=0.5\n",
        ")\n",
        "\n",
        "# Plot cluster centers\n",
        "centers_2d = pca_2d.transform(kmeans.cluster_centers_)\n",
        "plt.scatter(\n",
        "    centers_2d[:, 0], \n",
        "    centers_2d[:, 1], \n",
        "    c='red', \n",
        "    marker='X', \n",
        "    s=300, \n",
        "    edgecolors='black',\n",
        "    linewidth=2,\n",
        "    label='Centroids'\n",
        ")\n",
        "\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.xlabel(f'PCA Component 1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)', fontsize=12)\n",
        "plt.ylabel(f'PCA Component 2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)', fontsize=12)\n",
        "plt.title('Text Clustering Visualization (PCA 2D)', fontsize=16, fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('clustering_pca_2d.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive 2D visualization with Plotly\n",
        "fig = px.scatter(\n",
        "    df, \n",
        "    x='pca_1', \n",
        "    y='pca_2', \n",
        "    color='cluster',\n",
        "    hover_data=['rating', 'review'],\n",
        "    title='Interactive Text Clustering Visualization (PCA 2D)',\n",
        "    labels={\n",
        "        'pca_1': f'PCA 1 ({pca_2d.explained_variance_ratio_[0]:.2%})',\n",
        "        'pca_2': f'PCA 2 ({pca_2d.explained_variance_ratio_[1]:.2%})',\n",
        "        'cluster': 'Cluster'\n",
        "    },\n",
        "    color_continuous_scale='viridis',\n",
        "    width=1000,\n",
        "    height=700\n",
        ")\n",
        "\n",
        "# Add centroids\n",
        "fig.add_trace(go.Scatter(\n",
        "    x=centers_2d[:, 0],\n",
        "    y=centers_2d[:, 1],\n",
        "    mode='markers',\n",
        "    marker=dict(size=15, color='red', symbol='x', line=dict(width=2, color='black')),\n",
        "    name='Centroids',\n",
        "    showlegend=True\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    font=dict(size=12),\n",
        "    title_font=dict(size=18, family='Arial, bold')\n",
        ")\n",
        "\n",
        "fig.write_html('clustering_interactive_2d.html')\n",
        "fig.show()\n",
        "\n",
        "print(\"Interactive plot saved as 'clustering_interactive_2d.html'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3D visualization with PCA\n",
        "print(\"Applying PCA for 3D visualization...\")\n",
        "pca_3d = PCA(n_components=3, random_state=42)\n",
        "pca_3d_result = pca_3d.fit_transform(embeddings)\n",
        "\n",
        "df['pca_3d_1'] = pca_3d_result[:, 0]\n",
        "df['pca_3d_2'] = pca_3d_result[:, 1]\n",
        "df['pca_3d_3'] = pca_3d_result[:, 2]\n",
        "\n",
        "print(f\"3D Explained variance ratio: {pca_3d.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca_3d.explained_variance_ratio_):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive 3D visualization\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    x='pca_3d_1',\n",
        "    y='pca_3d_2',\n",
        "    z='pca_3d_3',\n",
        "    color='cluster',\n",
        "    hover_data=['rating', 'review'],\n",
        "    title='Interactive 3D Text Clustering Visualization',\n",
        "    labels={\n",
        "        'pca_3d_1': f'PCA 1 ({pca_3d.explained_variance_ratio_[0]:.2%})',\n",
        "        'pca_3d_2': f'PCA 2 ({pca_3d.explained_variance_ratio_[1]:.2%})',\n",
        "        'pca_3d_3': f'PCA 3 ({pca_3d.explained_variance_ratio_[2]:.2%})',\n",
        "        'cluster': 'Cluster'\n",
        "    },\n",
        "    color_continuous_scale='viridis',\n",
        "    width=1200,\n",
        "    height=800\n",
        ")\n",
        "\n",
        "# Add centroids in 3D\n",
        "centers_3d = pca_3d.transform(kmeans.cluster_centers_)\n",
        "fig.add_trace(go.Scatter3d(\n",
        "    x=centers_3d[:, 0],\n",
        "    y=centers_3d[:, 1],\n",
        "    z=centers_3d[:, 2],\n",
        "    mode='markers',\n",
        "    marker=dict(size=10, color='red', symbol='x', line=dict(width=2, color='black')),\n",
        "    name='Centroids',\n",
        "    showlegend=True\n",
        "))\n",
        "\n",
        "fig.update_layout(\n",
        "    font=dict(size=12),\n",
        "    title_font=dict(size=18, family='Arial, bold')\n",
        ")\n",
        "\n",
        "fig.write_html('clustering_interactive_3d.html')\n",
        "fig.show()\n",
        "\n",
        "print(\"Interactive 3D plot saved as 'clustering_interactive_3d.html'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Silhouette Score Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate overall silhouette score\n",
        "overall_silhouette_score = silhouette_score(embeddings, df['cluster'])\n",
        "print(f\"Overall Silhouette Score: {overall_silhouette_score:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "if overall_silhouette_score > 0.7:\n",
        "    print(\"Excellent clustering quality\")\n",
        "elif overall_silhouette_score > 0.5:\n",
        "    print(\"Good clustering quality\")\n",
        "elif overall_silhouette_score > 0.3:\n",
        "    print(\"Moderate clustering quality\")\n",
        "else:\n",
        "    print(\"Poor clustering quality - clusters may be overlapping\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate silhouette scores for each sample\n",
        "sample_silhouette_values = silhouette_samples(embeddings, df['cluster'])\n",
        "df['silhouette_score'] = sample_silhouette_values\n",
        "\n",
        "# Silhouette score by cluster\n",
        "print(\"\\nAverage Silhouette Score per Cluster:\")\n",
        "for i in range(n_clusters):\n",
        "    cluster_silhouette_values = sample_silhouette_values[df['cluster'] == i]\n",
        "    avg_score = cluster_silhouette_values.mean()\n",
        "    print(f\"Cluster {i}: {avg_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Silhouette plot\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "\n",
        "y_lower = 10\n",
        "colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    # Aggregate silhouette scores for samples in cluster i\n",
        "    ith_cluster_silhouette_values = sample_silhouette_values[df['cluster'] == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "    \n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "    \n",
        "    ax.fill_betweenx(\n",
        "        np.arange(y_lower, y_upper),\n",
        "        0,\n",
        "        ith_cluster_silhouette_values,\n",
        "        facecolor=colors[i],\n",
        "        edgecolor=colors[i],\n",
        "        alpha=0.7\n",
        "    )\n",
        "    \n",
        "    # Label the silhouette plots with their cluster numbers\n",
        "    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontsize=12, fontweight='bold')\n",
        "    \n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "ax.set_xlabel('Silhouette Coefficient Values', fontsize=12)\n",
        "ax.set_ylabel('Cluster Label', fontsize=12)\n",
        "ax.set_title('Silhouette Plot for Each Cluster', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Add vertical line for average silhouette score\n",
        "ax.axvline(x=overall_silhouette_score, color='red', linestyle='--', linewidth=2, label=f'Average: {overall_silhouette_score:.4f}')\n",
        "ax.legend()\n",
        "\n",
        "ax.set_yticks([])\n",
        "ax.set_xlim([-0.1, 1])\n",
        "plt.tight_layout()\n",
        "plt.savefig('silhouette_plot.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"Silhouette plot saved as 'silhouette_plot.png'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Export Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create summary report\n",
        "summary = {\n",
        "    'Total Reviews': len(df),\n",
        "    'Number of Clusters': n_clusters,\n",
        "    'Overall Silhouette Score': overall_silhouette_score,\n",
        "    'Embedding Model': 'sentence-transformers/all-mpnet-base-v2',\n",
        "    'Embedding Dimension': embeddings.shape[1],\n",
        "    'PCA Variance Explained (2D)': sum(pca_2d.explained_variance_ratio_),\n",
        "    'PCA Variance Explained (3D)': sum(pca_3d.explained_variance_ratio_)\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTERING ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "for key, value in summary.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{key:.<40} {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"{key:.<40} {value}\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export results to CSV\n",
        "output_df = df[['date', 'review', 'rating', 'userName', 'cleaned_review', 'cluster', 'silhouette_score', 'pca_1', 'pca_2']].copy()\n",
        "output_df.to_csv('clustering_results.csv', index=False)\n",
        "print(\"\\nResults exported to 'clustering_results.csv'\")\n",
        "\n",
        "# Export cluster keywords\n",
        "keywords_df = pd.DataFrame([\n",
        "    {\n",
        "        'Cluster': cluster_id, \n",
        "        'Count': len(df[df['cluster'] == cluster_id]),\n",
        "        'Top_Keywords': ', '.join(keywords[:10])\n",
        "    }\n",
        "    for cluster_id, keywords in top_keywords.items()\n",
        "])\n",
        "keywords_df.to_csv('cluster_keywords.csv', index=False)\n",
        "print(\"Cluster keywords exported to 'cluster_keywords.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all generated files\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATED FILES\")\n",
        "print(\"=\"*80)\n",
        "generated_files = [\n",
        "    'wordcloud.png',\n",
        "    'top_words.png',\n",
        "    'elbow_silhouette.png',\n",
        "    'cluster_statistics.png',\n",
        "    'clustering_pca_2d.png',\n",
        "    'clustering_interactive_2d.html',\n",
        "    'clustering_interactive_3d.html',\n",
        "    'silhouette_plot.png',\n",
        "    'clustering_results.csv',\n",
        "    'cluster_keywords.csv'\n",
        "]\n",
        "\n",
        "for idx, file in enumerate(generated_files, 1):\n",
        "    print(f\"{idx:2d}. {file}\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n✅ Analysis complete! All results have been saved.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
